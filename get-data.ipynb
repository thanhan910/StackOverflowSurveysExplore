{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urljoin\n",
    "import shutil\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SURVEYS_DATABASE_PAGE_URL = 'https://survey.stackoverflow.co/'\n",
    "DATA_FOLDER_PATH = 'local/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_surveydata_links():\n",
    "    response = SESSION.get(SURVEYS_DATABASE_PAGE_URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    data_gps_tracks = soup.find_all('a', {'data-gps-track': True, 'data-year': True})\n",
    "    data_gps_tracks_links = [(urljoin(response.url, a['href']), a['data-year']) for a in data_gps_tracks]\n",
    "    return data_gps_tracks_links\n",
    "\n",
    "\n",
    "def download_surveydata_files(url: str, year):\n",
    "    \n",
    "    response = SESSION.get(url)\n",
    "    \n",
    "    # Extract the file type from the url\n",
    "    file_type = url.split('.')[-1]\n",
    "\n",
    "    # Assert that the file type is zip\n",
    "    assert file_type == 'zip', f'File type {file_type} is not supported'\n",
    "    \n",
    "    # Save the file\n",
    "    file_name = f'{year}.{file_type}'\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    # Extract the zip file\n",
    "    path_to_extract = os.path.join(DATA_FOLDER_PATH, year)\n",
    "    with zipfile.ZipFile(f'{year}.{file_type}', 'r') as zip_ref:\n",
    "        zip_ref.extractall(path_to_extract)\n",
    "    \n",
    "    # Remove the zip file\n",
    "    os.remove(file_name)\n",
    "\n",
    "    # Remove __MACOSX folder if it exists\n",
    "    macosx_folder_path = os.path.join(path_to_extract, '__MACOSX')\n",
    "    if os.path.exists(macosx_folder_path) and os.path.isdir(macosx_folder_path):\n",
    "        shutil.rmtree(macosx_folder_path)\n",
    "\n",
    "\n",
    "def download_all_survey_data():\n",
    "    data_gps_tracks_links = get_surveydata_links()\n",
    "    for link, year in tqdm(data_gps_tracks_links):\n",
    "        download_surveydata_files(link, year)\n",
    "        # 5s per file\n",
    "        # Remove __MACOSX folder\n",
    "\n",
    "download_all_survey_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2/14 [00:00<00:01, 11.70it/s]C:\\Users\\An\\AppData\\Local\\Temp\\ipykernel_11888\\3053088495.py:80: DtypeWarning: Columns (62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_content = pd.read_csv(file_path, skiprows=2, header=None)\n",
      " 29%|██▊       | 4/14 [00:00<00:01,  6.17it/s]C:\\Users\\An\\AppData\\Local\\Temp\\ipykernel_11888\\3053088495.py:80: DtypeWarning: Columns (5,108,121,196,197,198) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_content = pd.read_csv(file_path, skiprows=2, header=None)\n",
      " 50%|█████     | 7/14 [00:04<00:06,  1.09it/s]C:\\Users\\An\\AppData\\Local\\Temp\\ipykernel_11888\\3053088495.py:113: DtypeWarning: Columns (8,12,13,14,15,16,50,51,52,53,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_content = pd.read_csv(content_file_path)\n",
      "100%|██████████| 14/14 [00:21<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_csv_files():\n",
    "    csv_files : dict[int, list] = {}\n",
    "    data_folder_path_levels = len(DATA_FOLDER_PATH.split(os.sep))\n",
    "    for root, dirs, files in os.walk(DATA_FOLDER_PATH):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                folders = root.split(os.sep)\n",
    "                year = int(folders[data_folder_path_levels])\n",
    "                if year not in csv_files:\n",
    "                    csv_files[year] = []\n",
    "                # Skip __MACOSX\n",
    "                if '__MACOSX' in file_path:\n",
    "                    continue\n",
    "                csv_files[year].append(file_path)\n",
    "\n",
    "    for year in csv_files:\n",
    "        if year >= 2017:\n",
    "            assert len(csv_files[year]) == 2, f'Year {year} has {len(csv_files[year])} files'\n",
    "            # Assert that the 2 csv files are survey_results_schema.csv and survey_results_public.csv\n",
    "            for file in csv_files[year]:\n",
    "                assert file.endswith('survey_results_schema.csv') or file.endswith('survey_results_public.csv'), f'File {file} is not supported'\n",
    "        else:\n",
    "            assert len(csv_files[year]) == 1, f'Year {year} has {len(csv_files[year])} files'\n",
    "\n",
    "    return csv_files\n",
    "\n",
    "csv_files = get_csv_files()\n",
    "\n",
    "def get_2016_schema_table():\n",
    "\n",
    "    def get_readme_2016_filepath():\n",
    "        for root, dirs, files in os.walk(os.path.join(DATA_FOLDER_PATH, '2016')):\n",
    "            if '__MACOSX' in root:\n",
    "                continue\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    readme_file_path = os.path.join(root, file)\n",
    "                    return readme_file_path\n",
    "\n",
    "    readme_file_path : str = get_readme_2016_filepath()\n",
    "\n",
    "\n",
    "    with open(readme_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # Get all lines from \"Database schema:\" to the end\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'Database schema:' in line:\n",
    "            schema_table_lines = [line for line in lines[i+1:] if line.strip() != '']\n",
    "            break\n",
    "\n",
    "    schema_table_header = schema_table_lines[0].split(' --- ')\n",
    "    schema_table_header = [cell.strip().strip(\"'\").strip('\"') for cell in schema_table_header]\n",
    "    schema_table_rows = [line.split(' --- ') for line in schema_table_lines[1:]]\n",
    "    schema_table_rows = [[cell.strip().strip(\"'\").strip('\"') for cell in row] for row in schema_table_rows]\n",
    "    schema_table_rows = [['' if cell == 'N/A' else cell for cell in row] for row in schema_table_rows]\n",
    "\n",
    "    schema_table_rows = [['Respondent', '', ''],] + schema_table_rows\n",
    "\n",
    "    schema_table_df = pd.DataFrame(schema_table_rows, columns=schema_table_header)\n",
    "\n",
    "    return schema_table_df\n",
    "\n",
    "def get_dataframes(csv_files):\n",
    "\n",
    "    dataframes = {}\n",
    "\n",
    "    for year, files in tqdm(csv_files.items()):\n",
    "        dataframes[year] = {}\n",
    "        \n",
    "        if year <= 2015:\n",
    "            # If year <= 2015, the csv file has two top rows as headers\n",
    "            assert len(files) == 1\n",
    "            file_path = files[0]\n",
    "            try:\n",
    "                df_schema = pd.read_csv(file_path, nrows=2, header=None)\n",
    "            except UnicodeDecodeError:\n",
    "                df_schema = pd.read_csv(file_path, nrows=2, encoding='latin1', header=None)\n",
    "            try:\n",
    "                df_content = pd.read_csv(file_path, skiprows=2, header=None)\n",
    "            except UnicodeDecodeError:\n",
    "                df_content = pd.read_csv(file_path, skiprows=2, encoding='latin1', header=None)\n",
    "\n",
    "            df_schema = df_schema.transpose().reset_index(drop=False)\n",
    "            if year < 2015:\n",
    "                df_schema.rename(columns={0: 'Question', 1: 'Answer', 'index': 'Column'}, inplace=True)\n",
    "            else:\n",
    "                df_schema.rename(columns={0: 'Answer', 1: 'Question', 'index': 'Column'}, inplace=True)\n",
    "            dataframes[year] = {\n",
    "                'schema': df_schema,\n",
    "                'content': df_content,\n",
    "            }\n",
    "        elif year == 2016:\n",
    "            file_path = files[0]\n",
    "            df_schema = get_2016_schema_table()\n",
    "            df_content = pd.read_csv(file_path)\n",
    "            assert df_content['Unnamed: 0'].is_unique\n",
    "            assert 'Respondent' not in df_content.columns\n",
    "            df_content.rename(columns={'Unnamed: 0': 'Respondent'}, inplace=True)\n",
    "            dataframes[year] = {\n",
    "                'schema': df_schema,\n",
    "                'content': df_content,\n",
    "            }\n",
    "        else:\n",
    "            for file_path in files:\n",
    "                if 'survey_results_public' in file_path:\n",
    "                    content_file_path = file_path\n",
    "                else:\n",
    "                    assert 'survey_results_schema' in file_path\n",
    "                    schema_file_path = file_path\n",
    "            \n",
    "            df_schema = pd.read_csv(schema_file_path)\n",
    "            df_content = pd.read_csv(content_file_path)\n",
    "\n",
    "            dataframes[year] = {\n",
    "                'schema': df_schema,\n",
    "                'content': df_content,\n",
    "            }\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "# 2011 - 2014\n",
    "# 2015\n",
    "# 2016\n",
    "# 2017 - 2020\n",
    "# 2021 - 2024\n",
    "\n",
    "dataframes = get_dataframes(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(dataframes[2021]['schema'].columns == dataframes[2022]['schema'].columns)\n",
    "all(dataframes[2023]['schema'].columns == dataframes[2024]['schema'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[2023]['schema']['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[2024]['schema'][dataframes[2024]['schema']['type'] == 'CS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[2021]['content'][[c for c in dataframes[2021]['content'].columns if 'Language' in c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_unmatched_qnames(year, df_schema, df_content):\n",
    "    assert year >= 2021\n",
    "    unmatched_qnames = sorted(set(df_schema['qname']) - set(df_content.columns))\n",
    "    unmatched_columns = sorted(set(df_content.columns) - set(df_schema['qname']))\n",
    "    i = 0\n",
    "    j = 0\n",
    "    min_j_assessed = 0\n",
    "    i_isfound = False\n",
    "    columns_qname = {col:  None for col in unmatched_columns}\n",
    "    qname_columns = {qname: [] for qname in unmatched_qnames}\n",
    "    columns_qname_found = []\n",
    "    while (i < len(unmatched_qnames)) and (min_j_assessed < len(unmatched_columns)):\n",
    "        if unmatched_columns[j].startswith(unmatched_qnames[i]):\n",
    "            i_isfound = True\n",
    "            qname_columns[unmatched_qnames[i]].append(unmatched_columns[j])\n",
    "            columns_qname[unmatched_columns[j]] = unmatched_qnames[i]\n",
    "            j += 1\n",
    "            min_j_assessed = j\n",
    "        else:\n",
    "            if i_isfound:\n",
    "                i += 1\n",
    "                i_isfound = False\n",
    "            else:\n",
    "                j += 1\n",
    "            if not i_isfound and j == len(unmatched_columns):\n",
    "                j = min_j_assessed\n",
    "                i += 1\n",
    "    return qname_columns, columns_qname\n",
    "\n",
    "for year in range(2021, 2025):\n",
    "    qname_columns, columns_qname = match_unmatched_qnames(year, dataframes[year]['schema'], dataframes[year]['content'])\n",
    "    # assert columns_qname_notfound == ['ConvertedCompYearly', 'ResponseId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ConvertedCompYearly', 'ResponseId'],\n",
       " ['Frequency', 'JobSatPoints', 'Knowledge', 'SOTeamsUsage'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2024\n",
    "qname_columns, columns_qname = match_unmatched_qnames(year, dataframes[year]['schema'], dataframes[year]['content'])\n",
    "unmatched_columns = [col for col in columns_qname if columns_qname[col] is None]\n",
    "unmatched_qnames = [qname for qname in qname_columns if qname_columns[qname] == []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>qname</th>\n",
       "      <th>question</th>\n",
       "      <th>force_resp</th>\n",
       "      <th>type</th>\n",
       "      <th>selector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>QID337</td>\n",
       "      <td>JobSatPoints</td>\n",
       "      <td>Assign points to the following attributes of y...</td>\n",
       "      <td>False</td>\n",
       "      <td>CS</td>\n",
       "      <td>VRTL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid         qname                                           question  \\\n",
       "62  QID337  JobSatPoints  Assign points to the following attributes of y...   \n",
       "\n",
       "   force_resp type selector  \n",
       "62      False   CS     VRTL  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[year]['schema'][dataframes[year]['schema']['qname'] == 'JobSatPoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10       3-5 times a week\n",
       "12       1-2 times a week\n",
       "15       1-2 times a week\n",
       "18                  Never\n",
       "20                  Never\n",
       "               ...       \n",
       "65268    1-2 times a week\n",
       "65270    1-2 times a week\n",
       "65291    1-2 times a week\n",
       "65351               Never\n",
       "65412    1-2 times a week\n",
       "Name: Frequency_1, Length: 28369, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[year]['content'][dataframes[year]['content']['Frequency_1'].notna()]['Frequency_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ConvertedCompYearly', 'ResponseId']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col for col in columns_qname if columns_qname[col] is None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dataframes[2021]['content'].columns) - set(dataframes[2021]['schema']['qname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dataframes[2021]['schema']['qname']) - set(dataframes[2021]['content'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[2020]['schema']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dataframes[2024]['schema']['qname'].unique()) - set(dataframes[2024]['content'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[2024]['schema']['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dataframes[2024]['content'].columns) - set(dataframes[2024]['schema']['qname'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
