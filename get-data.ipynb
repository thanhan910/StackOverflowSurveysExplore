{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:44<00:00,  3.20s/it]\n",
      "100%|██████████| 14/14 [00:38<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urljoin\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SURVEYS_DATABASE_PAGE_URL = 'https://survey.stackoverflow.co/'\n",
    "DATA_FOLDER_PATH = 'local/data'\n",
    "\n",
    "DOWNLOAD = True\n",
    "\n",
    "def get_surveydata_links():\n",
    "    response = SESSION.get(SURVEYS_DATABASE_PAGE_URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    data_gps_tracks = soup.find_all('a', {'data-gps-track': True, 'data-year': True})\n",
    "    data_gps_tracks_links = [(urljoin(response.url, a['href']), a['data-year']) for a in data_gps_tracks]\n",
    "    return data_gps_tracks_links\n",
    "\n",
    "\n",
    "def download_surveydata_files(url: str, year):\n",
    "    \n",
    "    response = SESSION.get(url)\n",
    "    \n",
    "    # Extract the file type from the url\n",
    "    file_type = url.split('.')[-1]\n",
    "\n",
    "    # Assert that the file type is zip\n",
    "    assert file_type == 'zip', f'File type {file_type} is not supported'\n",
    "    \n",
    "    # Save the file\n",
    "    file_name = f'{year}.{file_type}'\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    # Extract the zip file\n",
    "    path_to_extract = os.path.join(DATA_FOLDER_PATH, year)\n",
    "    with zipfile.ZipFile(f'{year}.{file_type}', 'r') as zip_ref:\n",
    "        zip_ref.extractall(path_to_extract)\n",
    "    \n",
    "    # Remove the zip file\n",
    "    os.remove(file_name)\n",
    "\n",
    "    # Remove __MACOSX folder if it exists\n",
    "    macosx_folder_path = os.path.join(path_to_extract, '__MACOSX')\n",
    "    if os.path.exists(macosx_folder_path) and os.path.isdir(macosx_folder_path):\n",
    "        shutil.rmtree(macosx_folder_path)\n",
    "\n",
    "\n",
    "def download_all_survey_data():\n",
    "    data_gps_tracks_links = get_surveydata_links()\n",
    "    for link, year in tqdm(data_gps_tracks_links):\n",
    "        download_surveydata_files(link, year)\n",
    "        # 5s per file\n",
    "        # Remove __MACOSX folder\n",
    "\n",
    "if DOWNLOAD:\n",
    "    download_all_survey_data()\n",
    "\n",
    "def get_csv_files():\n",
    "    csv_files : dict[int, list] = {}\n",
    "    data_folder_path_levels = len(DATA_FOLDER_PATH.split(os.sep))\n",
    "    for root, dirs, files in os.walk(DATA_FOLDER_PATH):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                folders = root.split(os.sep)\n",
    "                year = int(folders[data_folder_path_levels])\n",
    "                if year not in csv_files:\n",
    "                    csv_files[year] = []\n",
    "                # Skip __MACOSX\n",
    "                if '__MACOSX' in file_path:\n",
    "                    continue\n",
    "                csv_files[year].append(file_path)\n",
    "\n",
    "    for year in csv_files:\n",
    "        if year >= 2017:\n",
    "            assert len(csv_files[year]) == 2, f'Year {year} has {len(csv_files[year])} files'\n",
    "            # Assert that the 2 csv files are survey_results_schema.csv and survey_results_public.csv\n",
    "            for file in csv_files[year]:\n",
    "                assert file.endswith('survey_results_schema.csv') or file.endswith('survey_results_public.csv'), f'File {file} is not supported'\n",
    "        else:\n",
    "            assert len(csv_files[year]) == 1, f'Year {year} has {len(csv_files[year])} files'\n",
    "\n",
    "    return csv_files\n",
    "\n",
    "csv_files = get_csv_files()\n",
    "\n",
    "def get_2016_schema_table():\n",
    "\n",
    "    def get_readme_2016_filepath():\n",
    "        for root, dirs, files in os.walk(os.path.join(DATA_FOLDER_PATH, '2016')):\n",
    "            if '__MACOSX' in root:\n",
    "                continue\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    readme_file_path = os.path.join(root, file)\n",
    "                    return readme_file_path\n",
    "\n",
    "    readme_file_path : str = get_readme_2016_filepath()\n",
    "\n",
    "\n",
    "    with open(readme_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # Get all lines from \"Database schema:\" to the end\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'Database schema:' in line:\n",
    "            schema_table_lines = [line for line in lines[i+1:] if line.strip() != '']\n",
    "            break\n",
    "\n",
    "    schema_table_header = schema_table_lines[0].split(' --- ')\n",
    "    schema_table_header = [cell.strip().strip(\"'\").strip('\"') for cell in schema_table_header]\n",
    "    schema_table_rows = [line.split(' --- ') for line in schema_table_lines[1:]]\n",
    "    schema_table_rows = [[cell.strip().strip(\"'\").strip('\"') for cell in row] for row in schema_table_rows]\n",
    "    schema_table_rows = [['' if cell == 'N/A' else cell for cell in row] for row in schema_table_rows]\n",
    "\n",
    "    schema_table_rows = [['Respondent', '', ''],] + schema_table_rows\n",
    "\n",
    "    schema_table_df = pd.DataFrame(schema_table_rows, columns=schema_table_header)\n",
    "\n",
    "    return schema_table_df\n",
    "\n",
    "def get_dataframes(csv_files):\n",
    "\n",
    "    dataframes = {}\n",
    "\n",
    "    for year, files in tqdm(csv_files.items()):\n",
    "        dataframes[year] = {}\n",
    "        \n",
    "        if year <= 2015:\n",
    "            # If year <= 2015, the csv file has two top rows as headers\n",
    "            assert len(files) == 1\n",
    "            file_path = files[0]\n",
    "            \n",
    "            try:\n",
    "                df_schema = pd.read_csv(file_path, nrows=2, header=None)\n",
    "            except UnicodeDecodeError:\n",
    "                df_schema = pd.read_csv(file_path, nrows=2, encoding='latin1', header=None)\n",
    "            try:\n",
    "                df_content = pd.read_csv(file_path, skiprows=2, header=None, low_memory=False)\n",
    "            except UnicodeDecodeError:\n",
    "                df_content = pd.read_csv(file_path, skiprows=2, encoding='latin1', header=None, low_memory=False)\n",
    "\n",
    "            df_schema = df_schema.transpose().reset_index(drop=False)\n",
    "\n",
    "            if year < 2015:\n",
    "                df_schema.rename(columns={0: 'Question', 1: 'Answer', 'index': 'Column'}, inplace=True)\n",
    "            else: # year == 2015\n",
    "                df_schema.rename(columns={0: 'Question Type', 1: 'Question', 'index': 'Column'}, inplace=True)\n",
    "            \n",
    "            dataframes[year] = {\n",
    "                'schema': df_schema,\n",
    "                'content': df_content,\n",
    "            }\n",
    "\n",
    "        elif year == 2016:\n",
    "            file_path = files[0]\n",
    "            df_schema = get_2016_schema_table()\n",
    "            df_content = pd.read_csv(file_path, low_memory=False)\n",
    "            assert df_content['Unnamed: 0'].is_unique\n",
    "            assert 'Respondent' not in df_content.columns\n",
    "            df_content.rename(columns={'Unnamed: 0': 'Respondent'}, inplace=True)\n",
    "            dataframes[year] = {\n",
    "                'schema': df_schema,\n",
    "                'content': df_content,\n",
    "            }\n",
    "        else:\n",
    "            for file_path in files:\n",
    "                if 'survey_results_public' in file_path:\n",
    "                    content_file_path = file_path\n",
    "                else:\n",
    "                    assert 'survey_results_schema' in file_path\n",
    "                    schema_file_path = file_path\n",
    "            \n",
    "            df_schema = pd.read_csv(schema_file_path)\n",
    "            df_content = pd.read_csv(content_file_path, low_memory=False)\n",
    "            dataframes[year] = {\n",
    "                'schema': df_schema,\n",
    "                'content': df_content,\n",
    "            }\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "# 2011 - 2014\n",
    "# 2015\n",
    "# 2016\n",
    "# 2017 - 2020\n",
    "# 2021 - 2024\n",
    "\n",
    "dataframes : dict[int, dict[str, pd.DataFrame]] = get_dataframes(csv_files)\n",
    "\n",
    "# Transform 2011 - 2014 dataframes\n",
    "for year in range(2011, 2015):\n",
    "    schema_rows = dataframes[year]['schema'].to_dict(orient='records')\n",
    "    it = 0\n",
    "    for r in schema_rows:\n",
    "        if(not pd.isna(r['Question'])):\n",
    "            it += 1\n",
    "        r['QID'] = f'Q{it}'\n",
    "    dataframes[year]['schema'] = pd.DataFrame(schema_rows, columns=['QID', 'Column', 'Question', 'Answer'])\n",
    "    dataframes[year]['questions'] = dataframes[year]['schema'][['QID', 'Question']].drop_duplicates(subset=['QID'], keep='first', inplace=False)\n",
    "    dataframes[year]['schema'].drop(columns=['Question'], inplace=True)\n",
    "    dataframes[year]['content'].columns = [f'C{col + 1}' for col in dataframes[year]['content'].columns]\n",
    "    dataframes[year]['schema']['Column'] = dataframes[2015]['schema']['Column'].apply(lambda x: f'C{x + 1}')\n",
    "\n",
    "# Transform 2015 dataframes\n",
    "dataframes[2015]['schema']['Answer'] = dataframes[2015]['schema']['Question'].str.split(': ').apply(lambda x: x[1] if len(x) > 1 else np.nan)\n",
    "dataframes[2015]['schema']['Question'] = dataframes[2015]['schema']['Question'].str.split(': ').apply(lambda x: x[0])\n",
    "dataframes[2015]['questions'] = dataframes[2015]['schema'].drop_duplicates(subset=['Question'], keep='first', inplace=False)[['Question', 'Question Type']]\n",
    "dataframes[2015]['questions'].reset_index(drop=True, inplace=True)\n",
    "dataframes[2015]['questions'].reset_index(drop=False, inplace=True)\n",
    "dataframes[2015]['questions'].rename(columns={'index': 'QID'}, inplace=True)\n",
    "dataframes[2015]['questions']['QID'] = dataframes[2015]['questions']['QID'].apply(lambda x: f'Q{x+1}')\n",
    "dataframes[2015]['schema'].reset_index(drop=False, inplace=True)\n",
    "dataframes[2015]['schema'].drop(columns=['Question Type'], inplace=True)\n",
    "dataframes[2015]['schema'] = pd.merge(dataframes[2015]['schema'], dataframes[2015]['questions'], on='Question', how='left', suffixes=('', '_y'))[['QID', 'Column', 'Answer']]\n",
    "\n",
    "dataframes[2015]['content'].columns = [f'C{col + 1}' for col in dataframes[2015]['content'].columns]\n",
    "dataframes[2015]['schema']['Column'] = dataframes[2015]['schema']['Column'].apply(lambda x: f'C{x + 1}')\n",
    "\n",
    "\n",
    "# Transform 2016 dataframes\n",
    "dataframes[2016]['schema']['Survey Question'] = dataframes[2016]['schema']['Survey Question'].replace('', np.nan)\n",
    "dataframes[2016]['questions'] = dataframes[2016]['schema']['Survey Question'].drop_duplicates(keep='first', inplace=False).dropna().reset_index(drop=True, inplace=False).reset_index(drop=False, inplace=False).rename(columns={'index': 'QID'}, inplace=False)\n",
    "dataframes[2016]['questions']['QID'] = dataframes[2016]['questions']['QID'].apply(lambda x: f'Q{x+1}')\n",
    "dataframes[2016]['schema'] = dataframes[2016]['schema'].merge(dataframes[2016]['questions'], left_on='Survey Question', right_on='Survey Question', how='left', suffixes=('', '_y'))[['QID', 'Column Name', 'Note (if any)']]\n",
    "\n",
    "\n",
    "def match_unmatched_qnames(year, df_schema, df_content):\n",
    "    \"\"\"\n",
    "    Some qnames in the schema dataframe are not found among the columns in the content dataframe because the columns contains the qnames with an answer as a suffix.\n",
    "    This function matches the unmatched qnames with the columns in the content dataframe.\n",
    "    \"\"\"\n",
    "    assert year >= 2021\n",
    "    unmatched_qnames = sorted(set(df_schema['qname']) - set(df_content.columns))\n",
    "    unmatched_columns = sorted(set(df_content.columns) - set(df_schema['qname']))\n",
    "    i = 0\n",
    "    j = 0\n",
    "    min_j_assessed = 0\n",
    "    i_isfound = False\n",
    "    columns_qname = {col:  None for col in unmatched_columns}\n",
    "    qname_columns = {qname: [] for qname in unmatched_qnames}\n",
    "    while (i < len(unmatched_qnames)) and (min_j_assessed < len(unmatched_columns)):\n",
    "        if unmatched_columns[j].startswith(unmatched_qnames[i]):\n",
    "            i_isfound = True\n",
    "            qname_columns[unmatched_qnames[i]].append(unmatched_columns[j])\n",
    "            columns_qname[unmatched_columns[j]] = unmatched_qnames[i]\n",
    "            j += 1\n",
    "            min_j_assessed = j\n",
    "        else:\n",
    "            if i_isfound:\n",
    "                i += 1\n",
    "                i_isfound = False\n",
    "            else:\n",
    "                j += 1\n",
    "            if not i_isfound and j == len(unmatched_columns):\n",
    "                j = min_j_assessed\n",
    "                i += 1\n",
    "    return qname_columns, columns_qname\n",
    "\n",
    "def get_qname_to_columns_df(year, df_schema, df_content):\n",
    "    qname_columns, columns_qname = match_unmatched_qnames(year, df_schema, df_content)\n",
    "    unmatched_columns = [col for col in columns_qname if columns_qname[col] is None]\n",
    "    unmatched_qnames = [qname for qname in qname_columns if qname_columns[qname] == []]\n",
    "    \n",
    "    # Some assertions\n",
    "    assert unmatched_columns == ['ConvertedCompYearly', 'ResponseId']\n",
    "\n",
    "    qname_columns_csv = [(qname, '|'.join([col.removesuffix(qname) for col in matched_columns])) for qname, matched_columns in qname_columns.items()]\n",
    "    \n",
    "    return pd.DataFrame(qname_columns_csv, columns=['qname', 'columns'])\n",
    "\n",
    "# Transform 2021 - 2024 dataframes\n",
    "for year in range(2021, 2025):\n",
    "    dataframes[year]['questions'] = dataframes[year]['schema'][['qid', 'qname']].drop_duplicates(subset=['qid'], keep='first', inplace=False).reset_index(drop=True, inplace=False)\n",
    "    dataframes[year]['qname_columns'] = get_qname_to_columns_df(year, dataframes[year]['schema'], dataframes[year]['content'])\n",
    "\n",
    "\n",
    "# Output the dataframes to csv files\n",
    "output_dir = 'local/output'\n",
    "for year in range(2011, 2025):\n",
    "    os.makedirs(os.path.join(output_dir, str(year)), exist_ok=True)\n",
    "    for name, df in dataframes[year].items():\n",
    "        df.to_csv(os.path.join(output_dir, str(year), f'{name}.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
